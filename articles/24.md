# 24日目
こんにちは！24日目を担当する[Hiyama](https://x.com/toru_data)です。

## 24日目の内容 :streamlit:

24日目では、LLMとのチャットをより快適にするため、チャットのStream機能を試してみます。また、ワーク4の総復習として、クイズを実装し、実際に挑戦してみます！

## ストリーミング機能の実装
LLMチャットのストリーミング機能とは、すべての文字列が生成されてからメッセージを表示するのではなく、生成された文字列から順に表示していく機能を指します。Streamlitも、Snowflake Cortexもこのストリーミングに対応しているため、試してみたいと思います。

まず、ローカルで実行するにあたって、[Day23](https://st-advent-calendar-2024.streamlit.app/?day=23)と同様の環境を用意し、[Day22](https://st-advent-calendar-2024.streamlit.app/?day=22)のコードを`main.py`に記述します。また、`extract_message_from_response`関数はもう使わないので削除してしまいましょう。その上で、セッション取得部分を次のように昨日学習した`st.connection`を使うように修正します。
```python
# Get the credentials
session = st.connection("snowflake").session()
```

続いて、`Complete`関数でチャットを送る部分を次のように修正します。
```python
    # LLMに履歴を含むリクエストを送信
    response = Complete(llm_model, st.session_state.messages, stream=True, session=session)
```

ここでは、`stream=True`というオプションと`session=session`というオプションを追加しました。前者はストリーミング機能を有効化するオプションで、後者はローカルで`Complete`関数を実装する際に必要となるセッションの明示的な指定に関するオプションです。

これにより、生成された順で文字列が`response`変数に格納されていきます。続いてはその描画について確認します。
```python
    # アシスタントのレスポンスを追加
    with st.chat_message("assistant"):
        full_response = st.write_stream(response)
    
    st.session_state.messages.append({"role": "assistant", "content": full_response})
```

ここでは、assistantのチャットメッセージ描画部分に、`st.write_stream(response)`という内容を表示しています。この`st.write_stream`関数では、ストリーミングの出力を表示させることができます。

また、この関数の返り値は、最後のチャットの返信が完成した内容となります。これを`full_response`変数に格納し、最後にセッションステートの`messages`に追加して完了です。

これで一度アプリを実行してみましょう。下図のように生成された順にLLMからの返信がストリーミング表示されたでしょうか？

![ストリーミング表示（途中）の様子](app/static/day24_1.png "ストリーミング表示（途中）の様子")

![ストリーミング表示（完了）の様子](app/static/day24_2.png "ストリーミング表示（完了）の様子")

修正方法が分からなくなってしまった場合は、[GitHubのソースコード main_streaming.py](https://github.com/Sakatoku/streamlit-advent-calendar-2024/blob/main/work4_day5/main_streaming.py)を確認するようにしてください。


## クイズの前準備（チャットエリアのコンテナ化）
今回は、チャットアプリの下にクイズアプリを実装しようと考えています。ここで、`st.chat_input`が、アプリの最下部に固定されているため、`st.chat_messages`と`st.chat_input`の間にクイズアプリが来るという少し不思議な状況になってしまいます。

下記の例で具体的な状況を確認してみましょう。先程のアプリに次のコードを追加してみてください。
```python
st.info("どこに表示されていますか？")
```

すると次のようなアプリの表示になってしまいます。

![チャットメッセージとチャットインプットの間に要素が挿入される様子](app/static/day24_3.png "チャットメッセージとチャットインプットの間に要素が挿入される様子")

### st.container
`st.container`関数によりチャットエリアをコンテナ化し、表示する要素の順序を制御できます。具体的には、チャットメッセージやチャットインプットを一つのコンテナ内にグループ化することで、チャットエリアと`st.info`が意図した順番で表示されるようになります。

やることは非常にシンプルです。次のようにチャットアプリを`with st.container`で括るだけで良いです。
```python
# Get the credentials
session = st.connection("snowflake").session()

# チャット欄をコンテナで括る
with st.container(border=True):
    # LLMモデルの選択肢
    LLM_OPTIONS = [
        "mistral-large2",
        "llama3.1-8b",
        "llama3.1-70b",
    ]

    # （以下略：チャットアプリ全体を括ります）

st.info("どこに表示されていますか？")
```

![st.containerを適用した様子](app/static/day24_4.png "st.containerを適用した様子")

まずはこれでチャットエリアと`st.info`を分離することに成功しました。しかし、今度はチャットインプットとチャットメッセージ表示部が上下逆になってしまいましたね。これは、コンテナ化したことで、チャットインプットが最下部に来なくなったためです。

そこで更に、チャットメッセージを予め`st.container`として定義しておき、その`st.container`にチャットメッセージの要素を追加していくことでチャットメッセージを先に表示し、チャットインプットをコンテナの最下部に表示させることができます。実際にコードを確認していきましょう。

まずは、メッセージ表示用のコンテナを作成します。
```python
    # メッセージ表示用のコンテナを作成しておく
    messages = st.container(height=500)

    # 過去の会話を表示
    for message in st.session_state.messages:
        with messages.chat_message(message["role"]):
            st.markdown(message["content"])
```
このように返り値を設定することで、`messages`に要素を追加していくことができます。例えば、`messages.chat_message()`のような形です。これにより、`messages = st.container()`を実行した場所で、StreamlitのUI要素を追加していくことができます。

続いては、チャットインプットとさらなるメッセージの追加部分です。
```python
    # 新しいプロンプトを送信
    if prompt := st.chat_input("Cortex LLMとのチャットを始めましょう"):
        # ユーザーのメッセージを追加
        st.session_state.messages.append({"role": "user", "content": prompt})
        with messages.chat_message("user"):
            st.markdown(prompt)

        # LLMに履歴を含むリクエストを送信
        response = Complete(llm_model, st.session_state.messages, stream=True, session=session)
        
        # アシスタントのレスポンスを追加
        with messages.chat_message("assistant"):
            full_response = st.write_stream(response)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})
```

ここは、先程と同じように`messages`インスタンスを使用して`messages.chat_message()`を記述していきます。これにより、`st.chat_input()`のあとに記述された内容であるにも関わらず、予め定義された場所にUI要素が追加されていきます。

このコードを実行した様子が、下図の通りです。
![st.chat_messagesにst.containerを適用した様子](app/static/day24_5.png "st.chat_messagesにst.containerを適用した様子")

この`st.container`を用いたUIの表示順序の制御は非常に重要な概念です。今回は複数の要素をコンテナに組み込みましたが、一つの要素で良い場合は`st.empty()`といった関数も存在し、実際によく使う関数の一つになっています。


修正方法が分からなくなってしまった場合は、[GitHubのソースコード main.py](https://github.com/Sakatoku/streamlit-advent-calendar-2024/blob/main/work4_day5/main.py)を確認するようにしてください。



## クイズへの挑戦
クイズは、次のコードをコピー＆ペーストでアプリに追加してみましょう。
```python
# --- 全体に対するクイズを追加 ---
with st.expander("📝 Quiz: このアプリの仕組みを理解できているか確認してみましょう！（全3問）"):
    st.markdown("次の3問について、下の回答番号をそれぞれ選んでください。最後に一括で回答をチェックします。")

    # Q1
    st.markdown("**Q1. 最も大きなパラメータ数を持ち、多言語にも対応しているLLMモデルはどれでしょうか？**")
    st.write("""
    1. mistral-large2  
    2. llama3.1-8b  
    3. llama3.1-70b
    """)
    q1_idx = st.radio("回答番号を選択 (Q1)", ["1", "2", "3"], key="q1", index=None)

    # Q2
    st.markdown("**Q2. メッセージ用コンテナ`messages = st.container(height=500)`を用意することで、どのようなメリットが得られるでしょうか？**")
    st.write("""
    1. LLMの処理速度が速くなる  
    2. 環境依存を減らし開発やデプロイを効率的に行えるようになる
    3. 予め表示する要素を定義しておくことで、複数の要素を意図した順序で表示できるようになる
    """)
    q2_idx = st.radio("回答番号を選択 (Q2)", ["1", "2", "3"], key="q2", index=None)

    # Q3
    st.markdown("**Q3. LLMからのストリーム対応を行うには、どのコードが正しいでしょうか？**")
    st.write("""
    1. `response = Complete(llm_model, st.session_state.messages, session=session)`  
    2. `response = Complete(llm_model, st.session_state.messages, stream=True, session=session)`  
    3. `response = Complete(llm_model, st.session_state.messages, stream=False, session=session)`
    """)
    q3_idx = st.radio("回答番号を選択 (Q3)", ["1", "2", "3"], key="q3", index=None)

    # 一括回答チェック
    if st.button("クイズの回答をチェック"):
        # Q1
        if q1_idx == "1":
            st.success("Q1: 正解です！`mistral-large2` は最も大きなパラメータ数（120B）を持ち、多言語対応も優れています。")
        else:
            st.error("Q1: 残念！もう一度挑戦してみましょう。")

        # Q2
        if q2_idx == "3":
            st.success("Q2: 正解です！メッセージ用コンテナを用意することで、複数の要素を意図した順序で表示できます。")
        else:
            st.error("Q2: 残念！もう一度挑戦してみましょう。")

        # Q3
        if q3_idx == "2":
            st.success("Q3: 正解です！ストリーム対応を行うには `stream=True` を指定します。")
        else:
            st.error("Q3: 残念！もう一度挑戦してみましょう。")
```

クイズについては、アプリから挑戦してみて確認してみてください。また、分からない問題は、チャットに問い合わせてみましょう。LLMが適切な内容を返せるのか、返せない場合はなぜなのか考えてみるのも面白いかもしれませんね！（ヒント：利用しているモデルの学習データが用意された時期はいつか？など）


今日の内容は少し難しかったかもしれません。完全に理解できていなくても問題ないので、こういう概念があるんだなと知ってもらえれば大丈夫です。何はともあれ、ワーク4 チャットアプリ編は以上となります。お疲れ様でした！

また、今回のワークでは触れませんでしたが、SnowflakeではRAGを簡単に構築できる機能「[Cortex Search](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-search/cortex-search-overview)」や、自然言語でデータについて問い合わせるとその結果やグラフを表示できる「[Cortex Analyst](https://docs.snowflake.com/en/user-guide/snowflake-cortex/cortex-analyst)」といった機能があります。これらについてもぜひ学んでみてください！

## 次回予告
次回はいよいよ最終日です。どんな試練があなたを待ち受けているのでしょうか？次回もお楽しみに！:streamlit:

