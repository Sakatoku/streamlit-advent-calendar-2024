# 21日目
こんにちは！21日目を担当する[ishihara](https://ex-ture.com/blog/author/ishiharaex-ture-jp/)です。

## 20日目のおさらい :streamlit:

20日目ではStreamlit in Snowflakeの環境上でSnowflake ML Pythonライブラリを使用して様々なLLM関数を実行してみました。

本日も前回に引き続きCortexライブラリを使って、アプリを作成していきます。

## シングルチャットアプリの作成

前回設定していただいたStreamlit in Snowflakeの環境を使って、シングルチャットアプリを作成していきます。
前回同様にSnowflake ML Pythonライブラリを使用していきますのでライブラリを選択し、インストールしておきましょう。

![Snowflake ML Pythonライブラリのインストール](app/static/day21_1.png "Snowflake ML Pythonライブラリのインストール")


それでは、早速Snowflakeのアプリの方作成していきます。

```python
import streamlit as st
from snowflake.cortex import Complete
from snowflake.snowpark.context import get_active_session

# Get the current credentials
session = get_active_session()

# LLMモデルの選択肢
LLM_OPTIONS = [
    "mistral-large2",
    "llama3.1-8b",
    "llama3.1-70b",
]

# LLMモデルの選択
llm_model = st.selectbox("対話するCortex LLMモデルを選択してください", LLM_OPTIONS)

# 新しいプロンプトを送信
if prompt := st.chat_input("Cortex LLMとのチャットを始めましょう"):
    # ユーザーのメッセージを追加
    with st.chat_message("user"):
        st.markdown(prompt)

    # LLMにリクエストを送信
    response = Complete(llm_model, prompt)

    # アシスタントのレスポンスを追加
    with st.chat_message("assistant"):
        st.markdown(response)
```

上記を実行すると、アプリが表示されます。

![シングルチャットアプリ実行](app/static/day21_2.png "シングルチャットアプリの実行")

実際にテキストを入力してみましょう。

![mistral-large2結果](app/static/day21_3.png "mistral-large2結果")

それではセレクトボックスから選択したLLMモデルによって、返答が変わることを確認してみましょう。
今回デフォルトで設定されているモデルは```mistral-large2```でした。

次は```llama3.1-8b```を選択してみましょう。

![llama3.1-8b結果](app/static/day21_4.png "llama3.1-8b結果")

結果の方かなりシンプルな返答になりましたね。

最後に```llama3.1-70b```を選択してみましょう。

![llama3.1-70b結果](app/static/day21_5.png "llama3.1-70b結果")

こちらは自分の名前を把握していることがわかるような返答になりました。
このように、選択したLLMモデルによって返答が変わることが分かります。

## モデルについて

### mistral-large2
```mistral-large2```は、フランスのMistral AI社が開発したモデルで、多言語対応やコーディングなどの質問も対応しているモデルです。

パラメータサイズは123B（1230億）となっています。

性能のベンチマークとしては、OpenAIの```GPT-4o```や、Anthropicの```Claude 3 Opus```、Metaの```Llama 3.1-405B```といったモデルと同程度のコード生成能力を持っています。

今回の例として使用したモデルの中では最も性能が高いモデルになっています。

### llama3.1-8b
```llama3.1-8b```は、Metaが開発したモデルで、比較的小さなモデルです。
パラメータサイズは8Bとなっています。
性能としては```GPT-3.5 Turbo```と同程度とされています。

### llama3.1-70b
```llama3.1-70b```は、```llama3.1-8b```より大きなモデルです。
パラメータサイズは70Bとなっています。

### その他のモデル

今回は```mistral-large2```、```llama3.1-8b```、```llama3.1-70b```を使用しましたが、他にも様々なモデルが存在します。

使用可能なモデルの一覧は、[Snowflake Cortexの公式ドキュメント](https://docs.snowflake.com/en/sql-reference/functions/complete-snowflake-cortex#arguments)から確認できます。

例えば、GoogleのオープンLLMである```gemma-7b```や、シンガポールのReka社の```reka-core```、Snowflake独自のLLMモデルである```snowflake-arctic```などを使用することが可能です。

```mistral-large2```と同程度の性能を持つ```llama3.1-405b```もモデルの一覧へ追加することで使用可能になりますが、お使いのSnowflakeのリージョン設定によっては、クロスリージョンの設定をする必要があるモデルや関数もありますので、ご注意ください。
（参考：[Cortex LLM](https://docs.snowflake.com/en/user-guide/snowflake-cortex/llm-functions#availability)）

アプリの開発初期では高いパラメータサイズを持つモデルを使用しながら機能の確認やテストを行い、アプリの完成後には小さなモデルで性能が落ちないか確認し、本番では最適なモデルを選択するといった方法を取ることもできます。

LLMのアプリケーション開発を実施する際は様々なモデルの選択肢からより良いモデルを選択する必要がありますね。

## まとめ

今回作成したアプリは1度の質問に対して1つの回答を行うシンプルなシングルチャットと呼ばれるアプリです。

入力した内容を記録して会話できる機能をもったマルチターンチャットアプリについては次回以降のお楽しみとなります。

## 次回予告
本日は、Snowflakeの環境上でCortexライブラリを使ってシングルチャットアプリを作成とモデルの簡単なご紹介をしました。

次回は、マルチターンチャットアプリを作成していきます。お楽しみに！