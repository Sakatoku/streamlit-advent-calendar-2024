import streamlit as st
from snowflake.cortex import Complete


# Get the credentials
session = st.connection("snowflake").session()

# ãƒãƒ£ãƒƒãƒˆæ¬„ã‚’ã‚³ãƒ³ãƒ†ãƒŠã§æ‹¬ã‚‹
with st.container(border=True):
    # LLMãƒ¢ãƒ‡ãƒ«ã®é¸æŠè‚¢
    LLM_OPTIONS = [
        "mistral-large2",
        "llama3.1-8b",
        "llama3.1-70b",
    ]

    # LLMãƒ¢ãƒ‡ãƒ«ã®é¸æŠ
    llm_model = st.selectbox("å¯¾è©±ã™ã‚‹Cortex LLMãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„", LLM_OPTIONS)

    # ä¼šè©±å±¥æ­´ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã§ä¿æŒ
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆã—ã¦ãŠã
    messages = st.container(height=500)

    # éå»ã®ä¼šè©±ã‚’è¡¨ç¤º
    for message in st.session_state.messages:
        with messages.chat_message(message["role"]):
            st.markdown(message["content"])

    # æ–°ã—ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡
    if prompt := st.chat_input("Cortex LLMã¨ã®ãƒãƒ£ãƒƒãƒˆã‚’å§‹ã‚ã¾ã—ã‚‡ã†"):
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¿½åŠ 
        st.session_state.messages.append({"role": "user", "content": prompt})
        with messages.chat_message("user"):
            st.markdown(prompt)

        # LLMã«å±¥æ­´ã‚’å«ã‚€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        response = Complete(llm_model, st.session_state.messages, stream=True, session=session)
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿½åŠ 
        with messages.chat_message("assistant"):
            full_response = st.write_stream(response)
        
        st.session_state.messages.append({"role": "assistant", "content": full_response})



# --- å…¨ä½“ã«å¯¾ã™ã‚‹ã‚¯ã‚¤ã‚ºã‚’è¿½åŠ  ---
with st.expander("ğŸ“ Quiz: ã“ã®ã‚¢ãƒ—ãƒªã®ä»•çµ„ã¿ã‚’ç†è§£ã§ãã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼ï¼ˆå…¨3å•ï¼‰"):
    st.markdown("æ¬¡ã®3å•ã«ã¤ã„ã¦ã€ä¸‹ã®å›ç­”ç•ªå·ã‚’ãã‚Œãã‚Œé¸ã‚“ã§ãã ã•ã„ã€‚æœ€å¾Œã«ä¸€æ‹¬ã§å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")

    # Q1
    st.markdown("**Q1. æœ€ã‚‚å¤§ããªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æŒã¡ã€å¤šè¨€èªã«ã‚‚å¯¾å¿œã—ã¦ã„ã‚‹LLMãƒ¢ãƒ‡ãƒ«ã¯ã©ã‚Œã§ã—ã‚‡ã†ã‹ï¼Ÿ**")
    st.write("""
    1. mistral-large2  
    2. llama3.1-8b  
    3. llama3.1-70b
    """)
    q1_idx = st.radio("å›ç­”ç•ªå·ã‚’é¸æŠ (Q1)", ["1", "2", "3"], key="q1", index=None)

    # Q2
    st.markdown("**Q2. ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã‚³ãƒ³ãƒ†ãƒŠ`messages = st.container(height=500)`ã‚’ç”¨æ„ã™ã‚‹ã“ã¨ã§ã€ã©ã®ã‚ˆã†ãªãƒ¡ãƒªãƒƒãƒˆãŒå¾—ã‚‰ã‚Œã‚‹ã§ã—ã‚‡ã†ã‹ï¼Ÿ**")
    st.write("""
    1. LLMã®å‡¦ç†é€Ÿåº¦ãŒé€Ÿããªã‚‹  
    2. ç’°å¢ƒä¾å­˜ã‚’æ¸›ã‚‰ã—é–‹ç™ºã‚„ãƒ‡ãƒ—ãƒ­ã‚¤ã‚’åŠ¹ç‡çš„ã«è¡Œãˆã‚‹ã‚ˆã†ã«ãªã‚‹
    3. äºˆã‚è¡¨ç¤ºã™ã‚‹è¦ç´ ã‚’å®šç¾©ã—ã¦ãŠãã“ã¨ã§ã€è¤‡æ•°ã®è¦ç´ ã‚’æ„å›³ã—ãŸé †åºã§è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ãªã‚‹
    """)
    q2_idx = st.radio("å›ç­”ç•ªå·ã‚’é¸æŠ (Q2)", ["1", "2", "3"], key="q2", index=None)

    # Q3
    st.markdown("**Q3. LLMã‹ã‚‰ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ å¯¾å¿œã‚’è¡Œã†ã«ã¯ã€ã©ã®ã‚³ãƒ¼ãƒ‰ãŒæ­£ã—ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ**")
    st.write("""
    1. `response = Complete(llm_model, st.session_state.messages, session=session)`  
    2. `response = Complete(llm_model, st.session_state.messages, stream=True, session=session)`  
    3. `response = Complete(llm_model, st.session_state.messages, stream=False, session=session)`
    """)
    q3_idx = st.radio("å›ç­”ç•ªå·ã‚’é¸æŠ (Q3)", ["1", "2", "3"], key="q3", index=None)

    # ä¸€æ‹¬å›ç­”ãƒã‚§ãƒƒã‚¯
    if st.button("ã‚¯ã‚¤ã‚ºã®å›ç­”ã‚’ãƒã‚§ãƒƒã‚¯"):
        # Q1
        if q1_idx == "1":
            st.success("Q1: æ­£è§£ã§ã™ï¼`mistral-large2` ã¯æœ€ã‚‚å¤§ããªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼ˆ120Bï¼‰ã‚’æŒã¡ã€å¤šè¨€èªå¯¾å¿œã‚‚å„ªã‚Œã¦ã„ã¾ã™ã€‚")
        else:
            st.error("Q1: æ®‹å¿µï¼ã‚‚ã†ä¸€åº¦æŒ‘æˆ¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚")

        # Q2
        if q2_idx == "3":
            st.success("Q2: æ­£è§£ã§ã™ï¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã‚³ãƒ³ãƒ†ãƒŠã‚’ç”¨æ„ã™ã‚‹ã“ã¨ã§ã€è¤‡æ•°ã®è¦ç´ ã‚’æ„å›³ã—ãŸé †åºã§è¡¨ç¤ºã§ãã¾ã™ã€‚")
        else:
            st.error("Q2: æ®‹å¿µï¼ã‚‚ã†ä¸€åº¦æŒ‘æˆ¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚")

        # Q3
        if q3_idx == "2":
            st.success("Q3: æ­£è§£ã§ã™ï¼ã‚¹ãƒˆãƒªãƒ¼ãƒ å¯¾å¿œã‚’è¡Œã†ã«ã¯ `stream=True` ã‚’æŒ‡å®šã—ã¾ã™ã€‚")
        else:
            st.error("Q3: æ®‹å¿µï¼ã‚‚ã†ä¸€åº¦æŒ‘æˆ¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚")